{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import erfinv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/model/.virtualenvs/keras/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/train.csv')\n",
    "y_train = X_train.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 58)\n",
      "21694\n",
      "0.0364475178592\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.sum()\n",
    "print y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Basically I removed *calc, added 1-hot to *cat features. Thats all I've done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_columns = X_train.columns[X_train.columns.str.contains('calc|id')]\n",
    "categorical_columns = X_train.columns[X_train.columns.str.contains('cat')]\n",
    "\n",
    "binary_cols = np.setdiff1d(\n",
    "    X_train.columns[X_train.columns.str.contains('bin')],\n",
    "    bad_columns\n",
    ")\n",
    "# for col in X_train.columns:\n",
    "#     column_vals = X_train[col].unique()\n",
    "#     if np.all(np.in1d(column_vals, [0,1])):\n",
    "#         binary_cols.append(col)\n",
    "cols_to_scale = np.setdiff1d(X_train.columns, np.union1d(bad_columns, binary_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_categorical = X_train[categorical_columns]\n",
    "X_train_to_scale = X_train[cols_to_scale]\n",
    "X_train_binary = X_train[binary_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quantile_scaler = QuantileTransformer(output_distribution='normal')\n",
    "X_train_scaled = quantile_scaler.fit_transform(X_train_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class ShiftCategoricalsTransformer(TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.cols_to_add = X.columns.to_series().apply(lambda x: np.any(X[x] < 0))\n",
    "        self.cols_to_add = X.columns[self.cols_to_add]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        X_to_transform = X.copy()\n",
    "        X_to_transform[self.cols_to_add] += 1\n",
    "        return X_to_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### add 1 to columns with -1 values\n",
    "\n",
    "shift_categoricals = ShiftCategoricalsTransformer()\n",
    "X_train_categorical_shifted = shift_categoricals.fit_transform(X_train_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_categorical_ohe = ohe.fit_transform(X_train_categorical_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_full = np.hstack([X_train_scaled, X_train_categorical_ohe, X_train_binary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595212, 221)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SwapNoiseGenerator():\n",
    "    def __init__(self, data, input_swap_noise=0.15):\n",
    "        self.data = data\n",
    "        self.input_swap_noise = input_swap_noise\n",
    "        \n",
    "    def batch_generator(self, X, y=None, batch_size=32, return_y=False):\n",
    "        nrow = X.shape[0]\n",
    "        ncol = X.shape[1]\n",
    "        while True:\n",
    "            batch_indices = np.random.choice(nrow, batch_size, replace=False)\n",
    "            X_batch_output = X[batch_indices, :]\n",
    "            \n",
    "            replacement_mask = np.random.random(size=X_batch_output.shape) < self.input_swap_noise\n",
    "            replacement_row_indices = np.random.choice(nrow, (batch_size, ncol), replace=True)\n",
    "            replacement_col_indices = [np.arange(ncol)] * batch_size\n",
    "            replacement_matrix = X[replacement_row_indices, replacement_col_indices]\n",
    "            \n",
    "            X_batch_input = np.where(replacement_mask, replacement_matrix, X_batch_output)\n",
    "            if return_y:\n",
    "                y_batch = y[batch_indices]\n",
    "                yield [X_batch_input, y_batch], X_batch_output   \n",
    "            yield X_batch_input, X_batch_output            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_ = Input(shape=(221,))\n",
    "\n",
    "x1 = Dense(1500, activation='relu')(input_)\n",
    "x2 = Dense(1500, activation='relu')(x1)\n",
    "x3 = Dense(1500, activation='relu')(x2)\n",
    "output = Dense(221, activation='linear')(x3)\n",
    "\n",
    "autoencoder = Model(inputs=input_, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(\n",
    "    optimizer='adam',#optimizers.adam(decay=0.95), #optimizers.SGD(lr=0.03, decay=0.95),\n",
    "    loss='mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "STEPS_PER_EPOCH = X_train_full.shape[0]/BATCH_SIZE\n",
    "MAX_EPOCHS=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swap_noise_generator = SwapNoiseGenerator(None, 0.15)\n",
    "batch_generator = swap_noise_generator.batch_generator(X_train_full, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "4650/4650 [==============================] - 94s 20ms/step - loss: 0.1160\n",
      "Epoch 2/70\n",
      "4650/4650 [==============================] - 92s 20ms/step - loss: 0.0851\n",
      "Epoch 3/70\n",
      "4650/4650 [==============================] - 92s 20ms/step - loss: 0.0777\n",
      "Epoch 4/70\n",
      "4650/4650 [==============================] - 91s 20ms/step - loss: 0.0735\n",
      "Epoch 5/70\n",
      "4650/4650 [==============================] - 91s 20ms/step - loss: 0.0715\n",
      "Epoch 6/70\n",
      "4650/4650 [==============================] - 91s 20ms/step - loss: 0.0694\n",
      "Epoch 7/70\n",
      "4650/4650 [==============================] - 91s 20ms/step - loss: 0.0686\n",
      "Epoch 8/70\n",
      "4650/4650 [==============================] - 89s 19ms/step - loss: 0.0672\n",
      "Epoch 9/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0667\n",
      "Epoch 10/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0664\n",
      "Epoch 11/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0661\n",
      "Epoch 12/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0653\n",
      "Epoch 13/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0648\n",
      "Epoch 14/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0644\n",
      "Epoch 15/70\n",
      "4650/4650 [==============================] - 85s 18ms/step - loss: 0.0641\n",
      "Epoch 16/70\n",
      "4650/4650 [==============================] - 85s 18ms/step - loss: 0.0643\n",
      "Epoch 17/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0635\n",
      "Epoch 18/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0634\n",
      "Epoch 19/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0634\n",
      "Epoch 20/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0635\n",
      "Epoch 21/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0628\n",
      "Epoch 22/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0630\n",
      "Epoch 23/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0628\n",
      "Epoch 24/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0629\n",
      "Epoch 25/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0628\n",
      "Epoch 26/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0624\n",
      "Epoch 27/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0626\n",
      "Epoch 28/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0626\n",
      "Epoch 29/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0625\n",
      "Epoch 30/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0625\n",
      "Epoch 31/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0626\n",
      "Epoch 32/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0628\n",
      "Epoch 33/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0622\n",
      "Epoch 34/70\n",
      "4650/4650 [==============================] - 85s 18ms/step - loss: 0.0621\n",
      "Epoch 35/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0624\n",
      "Epoch 36/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0623\n",
      "Epoch 37/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0622\n",
      "Epoch 38/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0623\n",
      "Epoch 39/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0621\n",
      "Epoch 40/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0617\n",
      "Epoch 41/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0621\n",
      "Epoch 42/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0617\n",
      "Epoch 43/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0617\n",
      "Epoch 44/70\n",
      "4650/4650 [==============================] - 85s 18ms/step - loss: 0.0615\n",
      "Epoch 45/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0615\n",
      "Epoch 46/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0617\n",
      "Epoch 47/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0618\n",
      "Epoch 48/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0613\n",
      "Epoch 49/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0616\n",
      "Epoch 50/70\n",
      "4650/4650 [==============================] - 85s 18ms/step - loss: 0.0614\n",
      "Epoch 51/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0617\n",
      "Epoch 52/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0615\n",
      "Epoch 53/70\n",
      "4650/4650 [==============================] - 85s 18ms/step - loss: 0.0619\n",
      "Epoch 54/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0614\n",
      "Epoch 55/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0617\n",
      "Epoch 56/70\n",
      "4650/4650 [==============================] - 84s 18ms/step - loss: 0.0617\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f35dc644150>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit_generator(\n",
    "    generator=batch_generator, \n",
    "    epochs=MAX_EPOCHS, \n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=8, verbose=1, mode='auto')\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concatenated_encoder = Model(inputs=input_, outputs=[x1, x2, x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.save(filepath='/home/model/btopolski/DAE/autoencoder_model')\n",
    "concatenated_encoder.save(filepath='/home/model/btopolski/DAE/concatenated_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/model/.virtualenvs/keras/local/lib/python2.7/site-packages/keras/models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "concatenated_encoder = load_model('/home/model/btopolski/DAE/concatenated_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# def get_encoding_function(model, layer_indices):\n",
    "#     prediction_function = K.function(\n",
    "#         [model.input, K.learning_phase()], \n",
    "#         [model.layers[i].output for i in layer_indices]\n",
    "#     )\n",
    "    \n",
    "#     def prediction_concatenating_function(X):\n",
    "#         return np.hstack(prediction_function((X, False)))\n",
    "    \n",
    "#     return prediction_concatenating_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding_function = get_encoding_function(autoencoder, [1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_and_get_score(X_train, X_valid, y_train, y_valid, metrics=[metrics.log_loss, metrics.roc_auc_score], \n",
    "                              batch_size=128, max_epochs=70):\n",
    "    \n",
    "    input_ = Input(shape=(4500,))\n",
    "    x = Dropout(rate=0.1)(input_)\n",
    "    x = Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.05))(input_)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.05))(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.05))(x)\n",
    "\n",
    "    predictor = Model(inputs=input_, outputs=output)\n",
    "\n",
    "    predictor.compile(\n",
    "        optimizer=optimizers.adam(decay=0.005), #optimizers.SGD(lr=0.03, decay=0.95),\n",
    "        loss='binary_crossentropy',\n",
    "    )\n",
    "    \n",
    "    predictor.fit(\n",
    "        x=X_train,\n",
    "        y=y_train, \n",
    "        epochs=max_epochs, \n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, patience=5, verbose=1, mode='auto')\n",
    "        ],\n",
    "        validation_data=(X_valid,y_valid)\n",
    "    )\n",
    "    \n",
    "    predictions = predictor.predict(X_valid)\n",
    "    \n",
    "    results = {}\n",
    "    for fun in metrics:\n",
    "        results[fun.__name__] = fun(y_valid, predictions)    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_5fold = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_encoded = np.hstack(concatenated_encoder.predict(X_train_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for train_index, test_index in cv_5fold.split(X_train_encoded):\n",
    "    X_train_cv = X_train_encoded[train_index]\n",
    "    X_test_cv = X_train_encoded[test_index]\n",
    "    y_train_cv = y_train[train_index]\n",
    "    y_test_cv = y_train[test_index]\n",
    "    result = train_model_and_get_score(X_train_cv, X_test_cv, y_train_cv, y_test_cv, max_epochs=70)\n",
    "    results.append(result)\n",
    "    res_df = pd.DataFrame(results)\n",
    "    res_df.to_csv('/home/model/btopolski/DAE/cv_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df.log_loss.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
